{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12647efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geoph/Documents/python-related/torchenv/lib64/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchtext.models import ROBERTA_BASE_ENCODER\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de76196c-304f-4ae3-b5ae-363a3620d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba996b",
   "metadata": {},
   "source": [
    "### 1. Preparation of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1180ca",
   "metadata": {},
   "source": [
    "The unified dataset with rows for each pair of image names and corresponding texts is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07b0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts_path, images_path):\n",
    "    \"\"\"Create dataframe with path to image and corresponding text description\"\"\"\n",
    "\n",
    "    texts_df = pd.read_csv(texts_path)\n",
    "    texts_df['text'] = texts_df['color'] + \" \" + texts_df['name'] + \" \" + texts_df['description']\n",
    "    texts_df = texts_df[['Unnamed: 0','text']]\n",
    "    texts_df['product'] = np.arange(len(texts_df))\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"Image_id\", \"Image_name\",\"Text\",\"Product_id\"])  \n",
    "    \n",
    "    for i, image in enumerate(glob.glob(images_path)):\n",
    "        img_name = os.path.basename(image)\n",
    "        key_img_name = img_name.split('_')[0]\n",
    "        img_descr = texts_df[texts_df['Unnamed: 0']==int(key_img_name)].iloc[0,1:]\n",
    "        df = df.append({'Image_id':i, 'Image_name': img_name, 'Text':img_descr[0], 'Product_id':img_descr[1]}, ignore_index=True)\n",
    "    \n",
    "    return df, df['Product_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab63624",
   "metadata": {},
   "source": [
    "The class CustomImageLoader helps to build custom loader of the training data in the form of the batches. It takes into account that all the images, corresponding to the same product have to be in the same batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageLoader:\n",
    "    \"\"\"Creation of batches of images, texts and their ids.\"\"\"\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, batch_size=10):\n",
    "        self.img_labels = annotations_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def getbatch(self, prod_idx):\n",
    "        batch = []\n",
    "        sliced_indices = self.img_labels[self.img_labels['Product_id'].isin(prod_idx)].index\n",
    "\n",
    "        # print(f\"GETBATCH {prod_idx=}\\n{sliced_indices=}\\n\\n{self.img_labels[self.img_labels['Product_id'].isin(prod_idx)]}\")\n",
    "        for i in sliced_indices:\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[i, 1])\n",
    "            image = Image.open(img_path)\n",
    "            image_id, text, product_id = self.img_labels.iloc[i, 0], self.img_labels.iloc[i, 2], self.img_labels.iloc[i, 3]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            batch.append((image,text,(image_id, product_id)))\n",
    "            \n",
    "        unzipped = list(zip(*batch))\n",
    "        \n",
    "        return unzipped[0], unzipped[1], unzipped[2]\n",
    "    \n",
    "    def __getitem__(self, prod_idx):\n",
    "        return self.getbatch(prod_idx)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        products_groups = [products[i:i + self.batch_size] for i in range(0, len(products), self.batch_size)]\n",
    "        for i in products_groups:\n",
    "            yield self.getbatch(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c3778",
   "metadata": {},
   "source": [
    "Preparation steps for passing data to dataloader are made, as well as, the constant variables for the whole program are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97f3a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transformation of images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n",
    "])\n",
    "\n",
    "# Data paths (Arina)\n",
    "# descriptions_data = \"./processed_data/processedSKUs_nodups.csv\"\n",
    "# images_folder = \"./processed_data/images/*.jpg\"\n",
    "# img_dir = \"./processed_data/images/\"\n",
    "# George\n",
    "descriptions_data = \"../processedSKUs_nodups.csv\"\n",
    "images_folder = \"../images_onlyids/*.jpg\"\n",
    "img_dir = \"../images_onlyids/\"\n",
    "\n",
    "# Creation of organized dataframe\n",
    "annotations_file, products = create_dataset(descriptions_data, images_folder)\n",
    "\n",
    "# Creation of true constant adjacency matrix, where rows - products, columns - images. \n",
    "ADJACENCY_TRUE = pd.crosstab(annotations_file.Product_id, annotations_file.Image_id)\n",
    "# ADJACENCY_TRUE = np.zeros(ADJACENCY_TRUE.shape)\n",
    "# for p_id, img_id in zip(annotations_file.Product_id, annotations_file.Image_id):\n",
    "#     #print(f\"{p_id=}\\n{img_id=}\")\n",
    "#     ADJACENCY_TRUE[p_id, img_id] = 1\n",
    "#     # for img_id in annotations_file[annotations_file.Product_id == p_id].Image_id:\n",
    "#     #     ADJACENCY_TRUE[p_id, img_id]\n",
    "# ADJACENCY_TRUE = pd.DataFrame(ADJACENCY_TRUE)\n",
    "# ADJACENCY_TRUE.rename_axis(\"Image_id\", \"columns\", inplace=True)\n",
    "# ADJACENCY_TRUE.rename_axis(\"Product_id\", \"index\", inplace=True)\n",
    "\n",
    "# Creation of custom Batch Loader, where batch contains images, belonging to same product\n",
    "dataset = CustomImageLoader(annotations_file, img_dir, transform=transform)\n",
    "\n",
    "# Creation of batches of products. For each product there are 2/3 images, so the actual batch size is ~20-30 pairs.\n",
    "prod_batch_size = 10\n",
    "products_groups = [products[i:i + prod_batch_size] for i in range(0, len(products), prod_batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ac54b1-7ce0-4028-b975-e9f6a672ca9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, (239,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_file.Product_id.max(), annotations_file.Product_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896fbe9a-3639-4110-864a-c3294a44aa62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Image_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 694 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Image_id    0    1    2    3    4    5    6    7    8    9    ...  684  685  \\\n",
       "Product_id                                                    ...             \n",
       "0             1    1    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "1             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "2             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "3             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "4             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "234           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "235           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "236           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "237           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "238           0    0    0    0    0    0    0    0    0    0  ...    1    0   \n",
       "\n",
       "Image_id    686  687  688  689  690  691  692  693  \n",
       "Product_id                                          \n",
       "0             0    0    0    0    0    0    0    0  \n",
       "1             0    0    0    0    0    0    0    0  \n",
       "2             0    0    0    0    0    0    0    0  \n",
       "3             0    0    0    0    0    0    0    0  \n",
       "4             0    0    0    0    0    0    0    0  \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "234           0    0    0    0    0    0    0    0  \n",
       "235           0    0    0    0    0    0    0    0  \n",
       "236           0    0    0    0    0    0    0    0  \n",
       "237           0    0    0    0    0    0    0    0  \n",
       "238           0    0    0    0    0    0    0    0  \n",
       "\n",
       "[239 rows x 694 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJACENCY_TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b201f81d-1530-4285-a442-3a094b675395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_id</th>\n",
       "      <th>Image_name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>105_0..jpg</td>\n",
       "      <td>18 Kt Rose Gold First Steps Earrings Crafted w...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>105_1..jpg</td>\n",
       "      <td>18 Kt Rose Gold First Steps Earrings Crafted w...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>105_2..jpg</td>\n",
       "      <td>18 Kt Rose Gold First Steps Earrings Crafted w...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Image_id  Image_name                                               Text  \\\n",
       "17        17  105_0..jpg  18 Kt Rose Gold First Steps Earrings Crafted w...   \n",
       "115      115  105_1..jpg  18 Kt Rose Gold First Steps Earrings Crafted w...   \n",
       "297      297  105_2..jpg  18 Kt Rose Gold First Steps Earrings Crafted w...   \n",
       "\n",
       "    Product_id  \n",
       "17         100  \n",
       "115        100  \n",
       "297        100  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_file[annotations_file.Product_id == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b801a19-d684-40cc-94f3-829b266a44bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image_id\n",
       "17     1\n",
       "115    1\n",
       "297    1\n",
       "Name: 100, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJACENCY_TRUE.iloc[100,:][ADJACENCY_TRUE.iloc[100,:]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc391fd6-cfd3-4cea-bb57-0c0ecc550a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9daa9d0-2e18-4d57-a3db-46ff8edb8e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    216\n",
       "2     23\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJACENCY_TRUE.sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00d48df-b498-4781-a3f6-e7494b77225e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Image_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 694 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Image_id    0    1    2    3    4    5    6    7    8    9    ...  684  685  \\\n",
       "Product_id                                                    ...             \n",
       "0             1    1    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "1             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "2             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "3             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "4             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "234           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "235           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "236           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "237           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "238           0    0    0    0    0    0    0    0    0    0  ...    1    0   \n",
       "\n",
       "Image_id    686  687  688  689  690  691  692  693  \n",
       "Product_id                                          \n",
       "0             0    0    0    0    0    0    0    0  \n",
       "1             0    0    0    0    0    0    0    0  \n",
       "2             0    0    0    0    0    0    0    0  \n",
       "3             0    0    0    0    0    0    0    0  \n",
       "4             0    0    0    0    0    0    0    0  \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "234           0    0    0    0    0    0    0    0  \n",
       "235           0    0    0    0    0    0    0    0  \n",
       "236           0    0    0    0    0    0    0    0  \n",
       "237           0    0    0    0    0    0    0    0  \n",
       "238           0    0    0    0    0    0    0    0  \n",
       "\n",
       "[239 rows x 694 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJACENCY_TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80e50d",
   "metadata": {},
   "source": [
    "### 2. Definition of classes for image, text encoders and supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eddcd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    \"\"\"L2-normalization of features columns\"\"\"\n",
    "    norm = torch.pow(X, 2).sum(dim=1, keepdim=True).sqrt()\n",
    "    X = torch.div(X, norm)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a3d5b0-e0fd-4068-a65f-2a010918d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size, cnn_type):\n",
    "        \"\"\"Initializing parameters\"\"\"\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.embedding_size = embedding_size # Size of projected image\n",
    "        self.cnn = self.load_cnn(cnn_type)\n",
    "\n",
    "        # No need to finetune parameters = frozen layers\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replacing last fully connected layer with new one\n",
    "        self.fc = nn.Linear(self.cnn.classifier._modules['6'].in_features, embedding_size)\n",
    "        self.cnn.classifier = nn.Sequential(*list(self.cnn.classifier.children())[:-1])\n",
    "\n",
    "        # Initializing the weights of fully-connected layer, which makes projection to new space\n",
    "        self.initialization_weights()\n",
    "  \n",
    "    def load_cnn(self, cnn_type):\n",
    "        \"\"\"Loading pretrained model\"\"\"\n",
    "        model = models.__dict__[cnn_type](pretrained=True)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def initialization_weights(self):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        r = np.sqrt(6.) / np.sqrt(self.fc.in_features + self.fc.out_features)\n",
    "        self.fc.weight.data.uniform_(-r, r)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Creation of features\"\"\"\n",
    "        # Creation of embeddings\n",
    "        features = self.cnn(X)\n",
    "\n",
    "        # Normalization of embeddings\n",
    "        features = normalization(features)\n",
    "\n",
    "        # Projection to new space\n",
    "        features = self.fc(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa3e6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBERTA_OUT_DIM = 768\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"Initializing parameters\"\"\"\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.embedding_size = embedding_size # Size of projected text\n",
    "        self.roberta = ROBERTA_BASE_ENCODER.get_model()\n",
    "        self.transform = ROBERTA_BASE_ENCODER.transform()\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(ROBERTA_OUT_DIM, embedding_size)\n",
    "\n",
    "        # Initializing the weights of fully-connected layer, which makes projection to new space\n",
    "        self.initialization_weights()\n",
    "        \n",
    "    def _roberta_encode(self, batch):\n",
    "        transformed = self.transform(batch)\n",
    "        model_input = to_tensor(transformed, padding_value=1)\n",
    "        return self.roberta(model_input)\n",
    "\n",
    "    def initialization_weights(self):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        r = np.sqrt(6.) / np.sqrt(self.fc.in_features + self.fc.out_features)\n",
    "        self.fc.weight.data.uniform_(-r, r)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        \"\"\"Creation of features\"\"\"\n",
    "        # Creation of embeddings\n",
    "        features = self._roberta_encode(list(X))\n",
    "\n",
    "        # Normalization of embeddings\n",
    "        features = normalization(features)\n",
    "\n",
    "        # Projection to new space\n",
    "        features = self.fc(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8470af7",
   "metadata": {},
   "source": [
    "### 3. Definition of loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f30e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(img, text):\n",
    "    \"\"\"Similarity calculation\"\"\"\n",
    "    \n",
    "    return text.mm(img.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0469a3d4-3f25-47ba-9888-367157141172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Image_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Image_id    0   1   2   3   4   5   6   7   8   9   ...  40  41  42  43  44  \\\n",
       "Product_id                                          ...                       \n",
       "0            1   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "1            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "2            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "3            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "4            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "5            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "6            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "7            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "8            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "9            0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "\n",
       "Image_id    45  46  47  48  49  \n",
       "Product_id                      \n",
       "0            0   0   0   0   0  \n",
       "1            0   0   0   0   0  \n",
       "2            0   0   0   0   0  \n",
       "3            0   0   0   0   0  \n",
       "4            0   0   0   0   0  \n",
       "5            0   0   0   0   0  \n",
       "6            0   0   0   0   0  \n",
       "7            0   0   0   0   0  \n",
       "8            0   0   0   0   0  \n",
       "9            0   0   0   0   0  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJACENCY_TRUE.iloc[:10, :50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "311d09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Class for combined loss\"\"\"\n",
    "\n",
    "    def __init__(self, margin=0, lambda_coeff=0.1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "        # self.sim = similarity_score\n",
    "    \n",
    "    def _calc_penalty(self, similarities):\n",
    "        if isinstance(similarities, torch.Tensor):\n",
    "            similarities = similarities.detach().numpy()\n",
    "        adj_tmp = np.zeros_like(similarities)\n",
    "        adj_tmp[similarities.argmax(0), np.arange(similarities.shape[1])] = 1\n",
    "        images_mapped = adj_tmp.sum(1)\n",
    "        assert images_mapped.sum() == similarities.shape[1], f\"{images_mapped.shape=}, {similarities.shape=}\"\n",
    "        penalty = self.lambda_coeff * len(np.where(images_mapped==0)[0])\n",
    "        return penalty\n",
    "\n",
    "    def forward(self, image_emb, label_emb, img_product_ids):\n",
    "        # Image_emb - batch_size x embedding size\n",
    "        # Label_emb - number of unique products from batch x embedding size\n",
    "        # Img_product_ids - batch_size x 2, where in each row there is (image_index, product_index).\n",
    "        batch_size = image_emb.shape[0]\n",
    "        products_num = label_emb.shape[0]\n",
    "        \n",
    "        # Slicing from true adjacency matrix, prepared beforehand\n",
    "        adj_true = ADJACENCY_TRUE.iloc[img_product_ids[:,1]]\n",
    "        adj_true = adj_true.iloc[:,img_product_ids[:,0]]\n",
    "        \n",
    "        adj_true = adj_true[~adj_true.index.duplicated(keep=\"first\")]\n",
    "        # print(adj_true.sum(0).values)\n",
    "        # Creation of similarity matrix of size - number of unique products from batch x batch_size\n",
    "        sim_matrix = similarity_score(image_emb, label_emb)\n",
    "        \n",
    "        # Calculation of loss for each product\n",
    "        # 1st - For each product minimum similarity with true images is calculated\n",
    "        adj_true = torch.Tensor(adj_true.values)\n",
    "        product_true = sim_matrix*adj_true\n",
    "        product_min_sims = product_true.masked_fill(product_true == 0, np.inf).min(dim=1)[0]\n",
    "        \n",
    "        # 2nd - For each product maximum similarity score with image, not belonging to it\n",
    "        adj_true_inv = torch.where(adj_true==0, 1, 0)\n",
    "        product_false = sim_matrix*adj_true_inv\n",
    "        product_max_sims, product_max_sims_indices = product_false.max(axis=1)\n",
    "        \n",
    "        # 3d - For each product maximum similarity between its images and products, other than the true one, is found\n",
    "        not_product_max_sims = np.array([])\n",
    "        for p in range(products_num):\n",
    "            sim_p = (sim_matrix[np.arange(products_num)!=p,:]*adj_true[p,:]).max()\n",
    "            not_product_max_sims = np.append(not_product_max_sims,sim_p.detach())\n",
    "        not_product_max_sims = torch.Tensor(not_product_max_sims)\n",
    "        \n",
    "        # Caluculation of mean loss\n",
    "        first_hinge = self.margin+not_product_max_sims - product_min_sims\n",
    "        second_hinge = self.margin+product_max_sims - product_min_sims\n",
    "        loss = torch.where(first_hinge<0,0,first_hinge)+torch.where(second_hinge<0,0,second_hinge)\n",
    "        loss = torch.sum(loss)/products_num\n",
    "        #Regularizer (this is just a single value)\n",
    "        loss += self._calc_penalty(sim_matrix)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e98a90-ed64-4919-a561-310843690e10",
   "metadata": {},
   "source": [
    "### 4. Definition of class and functions for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddd83e",
   "metadata": {},
   "source": [
    "The JewelryClassifier class combines the procedure of creation of the embeddings for images and texts, as well as, calculates loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c79bf14-7ff4-4086-b62a-10c061376d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JewelryClassifier:\n",
    "    \"\"\"Class, unifying the embeddings creation and loss calculation.\"\"\"\n",
    "    def __init__(self, emb_size=128, grad_clip=2, learning_rate=0.01, *loss_args):\n",
    "        self.grad_clip = grad_clip\n",
    "        self.im_enc = ImageEncoder(emb_size, 'vgg19')\n",
    "        self.txt_enc = TextEncoder(emb_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.im_enc.cuda()\n",
    "            self.txt_enc.cuda()\n",
    "        self.criterion = CombinedLoss(*loss_args)\n",
    "        params = list(self.txt_enc.fc.parameters())\n",
    "        params += list(self.im_enc.fc.parameters())\n",
    "        # The image cnn is fine-tuned but roberta is not.\n",
    "        params += self.im_enc.cnn.parameters()\n",
    "        self.params = params\n",
    "        self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "        self.step = 0\n",
    "    \n",
    "    def state_dict(self):\n",
    "        state_dict = [self.im_enc.state_dict(), self.txt_enc.state_dict()]\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.im_enc.load_state_dict(state_dict[0])\n",
    "        self.txt_enc.load_state_dict(state_dict[1])\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def on_stage_start(self, stage):\n",
    "        if stage == \"TRAIN\":\n",
    "            self.im_enc.train()\n",
    "            self.txt_enc.train()\n",
    "        elif stage == \"VALID\":\n",
    "            self.im_enc.eval()\n",
    "            self.txt_enc.eval()\n",
    "    \n",
    "    def train(self):\n",
    "        return self.on_stage_start(\"TRAIN\")\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.on_stage_start(\"VALID\")\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def to_tensor(x, grad=True):\n",
    "    #     if not isinstance(x, torch.Tensor):\n",
    "    #         x = torch.Tensor(x, requires_grad=grad)\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         x = x.cuda()\n",
    "    #     return x\n",
    "    \n",
    "    def forward_emb(self, imgs, txts, lengths=None, grad=True):\n",
    "        \"\"\"Compute the image and text embeddings\"\"\"\n",
    "        # Set mini-batch dataset\n",
    "        #imgs = self.to_tensor(imgs, grad)\n",
    "        #txts = self.to_tensor(txts, grad)\n",
    "\n",
    "        # Forward\n",
    "        imgs_emb = self.im_enc(imgs)\n",
    "        txts_emb = self.txt_enc(txts, lengths)\n",
    "        return imgs_emb, txts_emb\n",
    "\n",
    "    def forward_loss(self, imgs_emb, txts_emb, img_product_ids):\n",
    "        \"\"\"Compute the loss given pairs of image and text embeddings and there ids\"\"\"\n",
    "        loss = self.criterion(imgs_emb, txts_emb, img_product_ids)\n",
    "        return loss\n",
    "\n",
    "    def train_emb(self, imgs, txts, img_product_ids):\n",
    "        \"\"\"One training step given images and captions.\"\"\"\n",
    "        self.step += 1\n",
    "\n",
    "        # compute the embeddings\n",
    "        imgs_emb, txts_emb = self.forward_emb(imgs, txts)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.forward_loss(imgs_emb, txts_emb.mean(axis=1), img_product_ids)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        if self.grad_clip > 0:\n",
    "            clip_grad_norm(self.params, self.grad_clip)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "702b8a5a-0d00-44a0-8a04-8919b9d18286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                                                  | 0/24 [00:02<?, ?it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(Y)\u001b[38;5;241m.\u001b[39munique() \u001b[38;5;66;03m# From descriptions of products only unique ones are  selected, preserving the order\u001b[39;00m\n\u001b[1;32m      7\u001b[0m img_product_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(IDS)\n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_product_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mJewelryClassifier.train_emb\u001b[0;34m(self, imgs, txts, img_product_ids)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# compute the embeddings\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m imgs_emb, txts_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# measure accuracy and record loss\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mJewelryClassifier.forward_emb\u001b[0;34m(self, imgs, txts, lengths, grad)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"Compute the image and text embeddings\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Set mini-batch dataset\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#imgs = self.to_tensor(imgs, grad)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#txts = self.to_tensor(txts, grad)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m imgs_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim_enc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m txts_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtxt_enc(txts, lengths)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m imgs_emb, txts_emb\n",
      "File \u001b[0;32m~/Documents/python-related/torchenv/lib64/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"Creation of features\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Creation of embeddings\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Normalization of embeddings\u001b[39;00m\n\u001b[1;32m     37\u001b[0m features \u001b[38;5;241m=\u001b[39m normalization(features)\n",
      "File \u001b[0;32m~/Documents/python-related/torchenv/lib64/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/python-related/torchenv/lib64/python3.10/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/python-related/torchenv/lib64/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/python-related/torchenv/lib64/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/python-related/torchenv/lib64/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/python-related/torchenv/lib64/python3.10/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/python-related/torchenv/lib64/python3.10/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# That cycle has to be used to receive batches !!!\n",
    "model = JewelryClassifier(128, 2, 0.01, 0.2, 0.1)\n",
    "for i in tqdm(products_groups):\n",
    "    X, Y, IDS = dataset.getbatch(i) # X=images, Y=descriptions of products, IDS=pairs of (image_id,product_id)\n",
    "    images = torch.stack(X, dim=0) # Images are converted to batched tensor\n",
    "    labels = pd.Series(Y).unique() # From descriptions of products only unique ones are  selected, preserving the order\n",
    "    img_product_ids = np.array(IDS)\n",
    "    loss = model.train_emb(images, labels, img_product_ids)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f6424f4-f018-4962-a40d-17bbe59e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, epochs, emb_size, grad_clip=2, use_valid=False, *loss_args):\n",
    "    \"\"\"The main training function.\"\"\"\n",
    "    model = JewelryClassifier(emb_size, grad_clip, *loss_args)\n",
    "    pbar = tqdm(range(1, epochs+1))\n",
    "    avg_loss = \"inf\"\n",
    "    val_loss = \"inf\"\n",
    "    def format_desc():\n",
    "        pbar.set_description(f\"AVG Loss={avg_loss}, Train Loss={loss}, Valid Loss={val_loss}  (epoch {epoch})\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        # TODO: Add learning rate scheduler\n",
    "        losses = []\n",
    "        # pbar = tqdm(enumerate(train_loader))\n",
    "        # for i, (im_batch, txt_batch, img_product_ids) in pbar:\n",
    "        for i, pr_group in tqdm(enumerate(products_groups)):\n",
    "            im_batch, txt_batch, img_product_ids = dataset.getbatch(pr_group)\n",
    "            im_batch = torch.stack(im_batch, dim=0) # Images are converted to batched tensor\n",
    "            txt_batch = pd.Series(txt_batch).unique() # From descriptions of products only unique ones are  selected, preserving the order\n",
    "            img_product_ids = np.array(img_product_ids)\n",
    "            loss = model.train_emb(im_batch, txt_batch, img_product_ids)\n",
    "            losses.append(loss)\n",
    "            format_desc()\n",
    "        avg_loss = loss = sum(losses)/len(losses)\n",
    "        format_desc()\n",
    "        if use_valid:\n",
    "            val_losses = []\n",
    "            for i, (im_batch, txt_batch, txt_lengths) in enumerate(val_loader):\n",
    "                model.eval()\n",
    "                im_emb, txt_emb = model.forward_emb(im_batch, txt_batch, txt_lengths, grad=False)\n",
    "                val_loss = model.forward_loss(im_emb, txt_emb)\n",
    "                val_losses.append(val_loss)\n",
    "                format_desc()\n",
    "            val_loss = sum(val_losses)/len(val_losses)\n",
    "            format_desc()\n",
    "        # TODO Checkpointing\n",
    "    # Save model\n",
    "    model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2ce68d1-d363-4533-b934-2c3ac6b24b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                   | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomImageLoader' object has no attribute 'get_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproducts_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, epochs, emb_size, grad_clip, use_valid, *loss_args)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# pbar = tqdm(enumerate(train_loader))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# for i, (im_batch, txt_batch, img_product_ids) in pbar:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pr_group \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(products_groups)):\n\u001b[0;32m---> 16\u001b[0m     im_batch, txt_batch, img_product_ids \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m(pr_group)\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_emb(im_batch, txt_batch, img_product_ids)\n\u001b[1;32m     18\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomImageLoader' object has no attribute 'get_batch'"
     ]
    }
   ],
   "source": [
    "train(products_groups, None, 2, 128, 2, False, 0.2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47657f-e9a3-4282-b5a0-4333ba92fd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch CPU",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
