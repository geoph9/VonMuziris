{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12647efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchtext.models import ROBERTA_BASE_ENCODER\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de76196c-304f-4ae3-b5ae-363a3620d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba996b",
   "metadata": {},
   "source": [
    "### 1. Preparation of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1180ca",
   "metadata": {},
   "source": [
    "The unified dataset with rows for each pair of image names and corresponding texts is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07b0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts_path, images_path):\n",
    "    \"\"\"Create dataframe with path to image and corresponding text description\"\"\"\n",
    "\n",
    "    texts_df = pd.read_csv(texts_path)\n",
    "    texts_df['text'] = texts_df['color'] + \" \" + texts_df['name'] + \" \" + texts_df['description']\n",
    "    texts_df = texts_df[['Unnamed: 0','text']]\n",
    "    texts_df['product'] = np.arange(len(texts_df))\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"Image_id\", \"Image_name\",\"Text\",\"Product_id\"])  \n",
    "    \n",
    "    for i, image in enumerate(glob.glob(images_path)):\n",
    "        img_name = os.path.basename(image)\n",
    "        key_img_name = img_name.split('_')[0]\n",
    "        img_descr = texts_df[texts_df['Unnamed: 0']==int(key_img_name)].iloc[0,1:]\n",
    "        df = df.append({'Image_id':i, 'Image_name': img_name, 'Text':img_descr[0], 'Product_id':img_descr[1]}, ignore_index=True)\n",
    "    \n",
    "    return df, df['Product_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab63624",
   "metadata": {},
   "source": [
    "The class CustomImageLoader helps to build custom loader of the training data in the form of the batches. It takes into account that all the images, corresponding to the same product have to be in the same batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageLoader:\n",
    "    \"\"\"Creation of batches of images, texts and their ids.\"\"\"\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, batch_size=10):\n",
    "        self.img_labels = annotations_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def getbatch(self, prod_idx):\n",
    "        batch = []\n",
    "        sliced_indices = self.img_labels[self.img_labels['Product_id'].isin(prod_idx)].index\n",
    "\n",
    "        # print(f\"GETBATCH {prod_idx=}\\n{sliced_indices=}\\n\\n{self.img_labels[self.img_labels['Product_id'].isin(prod_idx)]}\")\n",
    "        for i in sliced_indices:\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[i, 1])\n",
    "            image = Image.open(img_path)\n",
    "            image_id, text, product_id = self.img_labels.iloc[i, 0], self.img_labels.iloc[i, 2], self.img_labels.iloc[i, 3]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            batch.append((image,text,(image_id, product_id)))\n",
    "            \n",
    "        unzipped = list(zip(*batch))\n",
    "        \n",
    "        return unzipped[0], unzipped[1], unzipped[2]\n",
    "    \n",
    "    def __getitem__(self, prod_idx):\n",
    "        return self.getbatch(prod_idx)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        products_groups = [products[i:i + self.batch_size] for i in range(0, len(products), self.batch_size)]\n",
    "        for i in products_groups:\n",
    "            yield self.getbatch(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c3778",
   "metadata": {},
   "source": [
    "Preparation steps for passing data to dataloader are made, as well as, the constant variables for the whole program are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97f3a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transformation of images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n",
    "])\n",
    "\n",
    "# Data paths (Arina)\n",
    "# descriptions_data = \"./processed_data/processedSKUs_nodups.csv\"\n",
    "# images_folder = \"./processed_data/images/*.jpg\"\n",
    "# img_dir = \"./processed_data/images/\"\n",
    "# George\n",
    "descriptions_data = \"../processedSKUs_nodups.csv\"\n",
    "images_folder = \"../images_onlyids/*.jpg\"\n",
    "img_dir = \"../images_onlyids/\"\n",
    "\n",
    "# Creation of organized dataframe\n",
    "annotations_file, products = create_dataset(descriptions_data, images_folder)\n",
    "\n",
    "# Creation of true constant adjacency matrix, where rows - products, columns - images. \n",
    "ADJACENCY_TRUE = pd.crosstab(annotations_file.Product_id, annotations_file.Image_id)\n",
    "# ADJACENCY_TRUE = np.zeros(ADJACENCY_TRUE.shape)\n",
    "# for p_id, img_id in zip(annotations_file.Product_id, annotations_file.Image_id):\n",
    "#     #print(f\"{p_id=}\\n{img_id=}\")\n",
    "#     ADJACENCY_TRUE[p_id, img_id] = 1\n",
    "#     # for img_id in annotations_file[annotations_file.Product_id == p_id].Image_id:\n",
    "#     #     ADJACENCY_TRUE[p_id, img_id]\n",
    "# ADJACENCY_TRUE = pd.DataFrame(ADJACENCY_TRUE)\n",
    "# ADJACENCY_TRUE.rename_axis(\"Image_id\", \"columns\", inplace=True)\n",
    "# ADJACENCY_TRUE.rename_axis(\"Product_id\", \"index\", inplace=True)\n",
    "\n",
    "# Creation of custom Batch Loader, where batch contains images, belonging to same product\n",
    "dataset = CustomImageLoader(annotations_file, img_dir, transform=transform)\n",
    "\n",
    "# Creation of batches of products. For each product there are 2/3 images, so the actual batch size is ~20-30 pairs.\n",
    "prod_batch_size = 10\n",
    "products_groups = [products[i:i + prod_batch_size] for i in range(0, len(products), prod_batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ac54b1-7ce0-4028-b975-e9f6a672ca9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, (239,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_file.Product_id.max(), annotations_file.Product_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b201f81d-1530-4285-a442-3a094b675395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_id</th>\n",
       "      <th>Image_name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100_0..jpg</td>\n",
       "      <td>18 Kt Rose Gold First Steps Earrings Crafted w...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100_1..jpg</td>\n",
       "      <td>18 Kt Rose Gold First Steps Earrings Crafted w...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100_2..jpg</td>\n",
       "      <td>18 Kt Rose Gold First Steps Earrings Crafted w...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image_id  Image_name                                               Text  \\\n",
       "2        2  100_0..jpg  18 Kt Rose Gold First Steps Earrings Crafted w...   \n",
       "3        3  100_1..jpg  18 Kt Rose Gold First Steps Earrings Crafted w...   \n",
       "4        4  100_2..jpg  18 Kt Rose Gold First Steps Earrings Crafted w...   \n",
       "\n",
       "  Product_id  \n",
       "2        100  \n",
       "3        100  \n",
       "4        100  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_file[annotations_file.Product_id == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b801a19-d684-40cc-94f3-829b266a44bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image_id\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: 100, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJACENCY_TRUE.iloc[100,:][ADJACENCY_TRUE.iloc[100,:]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc391fd6-cfd3-4cea-bb57-0c0ecc550a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9daa9d0-2e18-4d57-a3db-46ff8edb8e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    216\n",
       "2     23\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJACENCY_TRUE.sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a00d48df-b498-4781-a3f6-e7494b77225e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Image_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows Ã— 694 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Image_id    0    1    2    3    4    5    6    7    8    9    ...  684  685  \\\n",
       "Product_id                                                    ...             \n",
       "0             1    1    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "1             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "2             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "3             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "4             0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "234           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "235           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "236           0    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
       "237           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "238           0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "\n",
       "Image_id    686  687  688  689  690  691  692  693  \n",
       "Product_id                                          \n",
       "0             0    0    0    0    0    0    0    0  \n",
       "1             0    0    0    0    0    0    0    0  \n",
       "2             0    0    0    0    0    0    0    0  \n",
       "3             0    0    0    0    0    0    0    0  \n",
       "4             0    0    0    0    0    0    0    0  \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "234           0    0    0    0    0    0    0    0  \n",
       "235           0    0    0    0    0    0    0    0  \n",
       "236           0    0    0    0    0    0    0    0  \n",
       "237           1    1    1    0    0    0    0    0  \n",
       "238           0    0    0    1    1    1    0    0  \n",
       "\n",
       "[239 rows x 694 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJACENCY_TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80e50d",
   "metadata": {},
   "source": [
    "### 2. Definition of classes for image, text encoders and supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eddcd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    \"\"\"L2-normalization of features columns\"\"\"\n",
    "    norm = torch.pow(X, 2).sum(dim=1, keepdim=True).sqrt()\n",
    "    X = torch.div(X, norm)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a3d5b0-e0fd-4068-a65f-2a010918d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size, cnn_type):\n",
    "        \"\"\"Initializing parameters\"\"\"\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.embedding_size = embedding_size # Size of projected image\n",
    "        self.cnn = self.load_cnn(cnn_type)\n",
    "\n",
    "        # No need to finetune parameters = frozen layers\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replacing last fully connected layer with new one\n",
    "        self.fc = nn.Linear(self.cnn.classifier._modules['6'].in_features, embedding_size)\n",
    "        self.cnn.classifier = nn.Sequential(*list(self.cnn.classifier.children())[:-1])\n",
    "\n",
    "        # Initializing the weights of fully-connected layer, which makes projection to new space\n",
    "        self.initialization_weights()\n",
    "  \n",
    "    def load_cnn(self, cnn_type):\n",
    "        \"\"\"Loading pretrained model\"\"\"\n",
    "        model = models.__dict__[cnn_type](pretrained=True)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def initialization_weights(self):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        r = np.sqrt(6.) / np.sqrt(self.fc.in_features + self.fc.out_features)\n",
    "        self.fc.weight.data.uniform_(-r, r)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Creation of features\"\"\"\n",
    "        # Creation of embeddings\n",
    "        features = self.cnn(X)\n",
    "\n",
    "        # Normalization of embeddings\n",
    "        features = normalization(features)\n",
    "\n",
    "        # Projection to new space\n",
    "        features = self.fc(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa3e6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBERTA_OUT_DIM = 768\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"Initializing parameters\"\"\"\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.embedding_size = embedding_size # Size of projected text\n",
    "        self.roberta = ROBERTA_BASE_ENCODER.get_model()\n",
    "        self.transform = ROBERTA_BASE_ENCODER.transform()\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(ROBERTA_OUT_DIM, embedding_size)\n",
    "\n",
    "        # Initializing the weights of fully-connected layer, which makes projection to new space\n",
    "        self.initialization_weights()\n",
    "        \n",
    "    def _roberta_encode(self, batch):\n",
    "        transformed = self.transform(batch)\n",
    "        model_input = to_tensor(transformed, padding_value=1)\n",
    "        return self.roberta(model_input)\n",
    "\n",
    "    def initialization_weights(self):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        r = np.sqrt(6.) / np.sqrt(self.fc.in_features + self.fc.out_features)\n",
    "        self.fc.weight.data.uniform_(-r, r)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        \"\"\"Creation of features\"\"\"\n",
    "        # Creation of embeddings\n",
    "        features = self._roberta_encode(list(X))\n",
    "\n",
    "        # Normalization of embeddings\n",
    "        features = normalization(features)\n",
    "\n",
    "        # Projection to new space\n",
    "        features = self.fc(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8470af7",
   "metadata": {},
   "source": [
    "### 3. Definition of loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f30e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(img, text):\n",
    "    \"\"\"Similarity calculation\"\"\"\n",
    "    \n",
    "    return text.mm(img.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "311d09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Class for combined loss\"\"\"\n",
    "\n",
    "    def __init__(self, margin=0, lambda_coeff=0.1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "        # self.sim = similarity_score\n",
    "    \n",
    "    def _calc_penalty(self, similarities):\n",
    "        if isinstance(similarities, torch.Tensor):\n",
    "            similarities = similarities.detach().numpy()\n",
    "        adj_tmp = np.zeros_like(similarities)\n",
    "        adj_tmp[similarities.argmax(0), np.arange(similarities.shape[1])] = 1\n",
    "        images_mapped = adj_tmp.sum(1)\n",
    "        assert images_mapped.sum() == similarities.shape[1], f\"{images_mapped.shape=}, {similarities.shape=}\"\n",
    "        penalty = self.lambda_coeff * len(np.where(images_mapped==0)[0])\n",
    "        return penalty\n",
    "    \n",
    "    @classmethod\n",
    "    def batch_specific_adj_matrix(cls, img_product_ids):\n",
    "        # Slicing from true adjacency matrix, prepared beforehand\n",
    "        adj_true = ADJACENCY_TRUE.iloc[img_product_ids[:,1]]\n",
    "        adj_true = adj_true.iloc[:,img_product_ids[:,0]]\n",
    "        \n",
    "        adj_true = adj_true[~adj_true.index.duplicated(keep=\"first\")]\n",
    "        return adj_true\n",
    "\n",
    "    def forward(self, image_emb, label_emb, img_product_ids):\n",
    "        # Image_emb - batch_size x embedding size\n",
    "        # Label_emb - number of unique products from batch x embedding size\n",
    "        # Img_product_ids - batch_size x 2, where in each row there is (image_index, product_index).\n",
    "        batch_size = image_emb.shape[0]\n",
    "        products_num = label_emb.shape[0]\n",
    "        \n",
    "        # Slicing from true adjacency matrix, prepared beforehand\n",
    "        adj_true = ADJACENCY_TRUE.iloc[img_product_ids[:,1]]\n",
    "        adj_true = adj_true.iloc[:,img_product_ids[:,0]]\n",
    "        \n",
    "        adj_true = adj_true[~adj_true.index.duplicated(keep=\"first\")]\n",
    "        # print(adj_true.sum(0).values)\n",
    "        # Creation of similarity matrix of size - number of unique products from batch x batch_size\n",
    "        sim_matrix = similarity_score(image_emb, label_emb)\n",
    "        \n",
    "        # Calculation of loss for each product\n",
    "        # 1st - For each product minimum similarity with true images is calculated\n",
    "        adj_true = torch.Tensor(adj_true.values)\n",
    "        product_true = sim_matrix*adj_true\n",
    "        product_min_sims = product_true.masked_fill(product_true == 0, np.inf).min(dim=1)[0]\n",
    "        \n",
    "        # 2nd - For each product maximum similarity score with image, not belonging to it\n",
    "        adj_true_inv = torch.where(adj_true==0, 1, 0)\n",
    "        product_false = sim_matrix*adj_true_inv\n",
    "        product_max_sims, product_max_sims_indices = product_false.max(axis=1)\n",
    "        \n",
    "        # 3d - For each product maximum similarity between its images and products, other than the true one, is found\n",
    "        not_product_max_sims = np.array([])\n",
    "        for p in range(products_num):\n",
    "            sim_p = (sim_matrix[np.arange(products_num)!=p,:]*adj_true[p,:]).max()\n",
    "            not_product_max_sims = np.append(not_product_max_sims,sim_p.detach())\n",
    "        not_product_max_sims = torch.Tensor(not_product_max_sims)\n",
    "        \n",
    "        # Caluculation of mean loss\n",
    "        first_hinge = self.margin+not_product_max_sims - product_min_sims\n",
    "        second_hinge = self.margin+product_max_sims - product_min_sims\n",
    "        loss = torch.where(first_hinge<0,0,first_hinge)+torch.where(second_hinge<0,0,second_hinge)\n",
    "        loss = torch.sum(loss)/products_num\n",
    "        #Regularizer (this is just a single value)\n",
    "        loss += self._calc_penalty(sim_matrix)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e98a90-ed64-4919-a561-310843690e10",
   "metadata": {},
   "source": [
    "### 4. Definition of class and functions for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddd83e",
   "metadata": {},
   "source": [
    "The JewelryClassifier class combines the procedure of creation of the embeddings for images and texts, as well as, calculates loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c79bf14-7ff4-4086-b62a-10c061376d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JewelryClassifier:\n",
    "    \"\"\"Class, unifying the embeddings creation and loss calculation.\"\"\"\n",
    "    def __init__(self, emb_size=128, grad_clip=2, learning_rate=0.01, *loss_args):\n",
    "        self.grad_clip = grad_clip\n",
    "        self.im_enc = ImageEncoder(emb_size, 'vgg19')\n",
    "        self.txt_enc = TextEncoder(emb_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.im_enc.cuda()\n",
    "            self.txt_enc.cuda()\n",
    "        self.criterion = CombinedLoss(*loss_args)\n",
    "        params = list(self.txt_enc.fc.parameters())\n",
    "        params += list(self.im_enc.fc.parameters())\n",
    "        # The image cnn is fine-tuned but roberta is not.\n",
    "        params += self.im_enc.cnn.parameters()\n",
    "        self.params = params\n",
    "        self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "        self.step = 0\n",
    "    \n",
    "    def state_dict(self):\n",
    "        state_dict = [self.im_enc.state_dict(), self.txt_enc.state_dict()]\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.im_enc.load_state_dict(state_dict[0])\n",
    "        self.txt_enc.load_state_dict(state_dict[1])\n",
    "    \n",
    "    def save(self, path, epoch=-1):\n",
    "        state = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': self.state_dict(),\n",
    "            'step': self.step,\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        print(\"=> loading checkpoint '{}'\".format(path))\n",
    "        checkpoint = torch.load(path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        self.load_state_dict(checkpoint['model'])\n",
    "        self.step = checkpoint['step']\n",
    "        return start_epoch\n",
    "    \n",
    "    def load_only_model(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.step = 0\n",
    "    \n",
    "    def on_stage_start(self, stage):\n",
    "        if stage == \"TRAIN\":\n",
    "            self.im_enc.train()\n",
    "            self.txt_enc.train()\n",
    "        elif stage == \"VALID\":\n",
    "            self.im_enc.eval()\n",
    "            self.txt_enc.eval()\n",
    "    \n",
    "    def train(self):\n",
    "        return self.on_stage_start(\"TRAIN\")\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.on_stage_start(\"VALID\")\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def to_tensor(x, grad=True):\n",
    "    #     if not isinstance(x, torch.Tensor):\n",
    "    #         x = torch.Tensor(x, requires_grad=grad)\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         x = x.cuda()\n",
    "    #     return x\n",
    "    \n",
    "    def forward_emb(self, imgs, txts, lengths=None, grad=True):\n",
    "        \"\"\"Compute the image and text embeddings\"\"\"\n",
    "        # Set mini-batch dataset\n",
    "        #imgs = self.to_tensor(imgs, grad)\n",
    "        #txts = self.to_tensor(txts, grad)\n",
    "\n",
    "        # Forward\n",
    "        imgs_emb = self.im_enc(imgs)\n",
    "        txts_emb = self.txt_enc(txts, lengths)\n",
    "        return imgs_emb, txts_emb\n",
    "\n",
    "    def forward_loss(self, imgs_emb, txts_emb, img_product_ids):\n",
    "        \"\"\"Compute the loss given pairs of image and text embeddings and there ids\"\"\"\n",
    "        loss = self.criterion(imgs_emb, txts_emb, img_product_ids)\n",
    "        return loss\n",
    "    \n",
    "    def _batch_accuracy_score(self, image_emb, label_emb, img_product_ids):\n",
    "        self.eval()\n",
    "        # compute the embeddings\n",
    "        imgs_emb, txts_emb = model.forward_emb(imgs, txts)\n",
    "        # sim_matrix: rows=products, columns=images\n",
    "        sim_matrix = similarity_score(imgs_emb, txts_emb.mean(axis=1))\n",
    "        \n",
    "        # The accuracy score is the normalized number of products that\n",
    "        # have been mapped to at least one correct image and no incorrect images.\n",
    "        # NOTE: This is on the batch level so it's not normalized. Check the accuracy_score\n",
    "        #       function for the normalized version.\n",
    "        batch_adj_true = CombinedLoss.batch_specific_adj_matrix(img_product_ids).values\n",
    "        if isinstance(sim_matrix, torch.Tensor):\n",
    "            sim_matrix = sim_matrix.detach().numpy()\n",
    "        batch_adj_pred = np.zeros_like(sim_matrix)\n",
    "        batch_adj_pred[sim_matrix.argmax(0), np.arange(sim_matrix.shape[1])] = 1\n",
    "        accuracies = []\n",
    "        for prod_id in range(len(batch_adj_true)):\n",
    "            true_imgs = batch_adj_true[prod_id, :]\n",
    "            pred_imgs = batch_adj_pred[prod_id, :]\n",
    "            # If at least one incorrect image is mapped to the product\n",
    "            # then the accuracy on this product is 0.\n",
    "            if -1 in true_imgs - pred_imgs:\n",
    "                accuracies.append(0)\n",
    "                continue\n",
    "            # If no image is mapped to the product, the accuracy is 0\n",
    "            if len(np.where(pred_imgs==1)[0]) == 0:\n",
    "                accuracies.append(0)\n",
    "                continue\n",
    "            accuracies.append(1)\n",
    "        return accuracies\n",
    "    \n",
    "    def accuracy_score(self, products_groups, data_set=dataset):\n",
    "        accuracoes = []\n",
    "        for i, pr_group in tqdm(enumerate(products_groups)):\n",
    "            im_batch, txt_batch, img_product_ids = data_set.getbatch(pr_group)\n",
    "            im_batch = torch.stack(im_batch, dim=0) # Images are converted to batched tensor\n",
    "\n",
    "            img_product_ids = np.array(img_product_ids) # Numpy array from (image,product) pairs\n",
    "            \n",
    "            labels, discovered_products = [], [] \n",
    "            for p, product in enumerate(img_product_ids[:,1]): # In that loop unique text batch is created\n",
    "                if product not in discovered_products:\n",
    "                    labels.append(txt_batch[p])\n",
    "                    discovered_products.append(product)\n",
    "            \n",
    "            batch_accs = self._batch_accuracy_score(im_batch, labels, img_product_ids)\n",
    "            accuracies += batch_accs\n",
    "        return sum(accuracies)/len(accuracies)\n",
    "\n",
    "    def train_emb(self, imgs, txts, img_product_ids):\n",
    "        \"\"\"One training step given images and captions.\"\"\"\n",
    "        self.step += 1\n",
    "\n",
    "        # compute the embeddings\n",
    "        imgs_emb, txts_emb = self.forward_emb(imgs, txts)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.forward_loss(imgs_emb, txts_emb.mean(axis=1), img_product_ids)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        if self.grad_clip > 0:\n",
    "            clip_grad_norm(self.params, self.grad_clip)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c49c1e6-02ae-449b-8c23-597f0a4caad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_accuracy_score(model, imgs, txts, img_product_ids):\n",
    "    model.eval()\n",
    "    # compute the embeddings\n",
    "    imgs_emb, txts_emb = model.forward_emb(imgs, txts)\n",
    "    # sim_matrix: rows=products, columns=images\n",
    "    sim_matrix = similarity_score(imgs_emb, txts_emb.mean(axis=1))\n",
    "    # The accuracy score is the normalized number of products that\n",
    "    # have been mapped to at least one correct image and no incorrect images.\n",
    "    # NOTE: This is on the batch level so it's not normalized. Check the accuracy_score\n",
    "    #       function for the normalized version.\n",
    "    batch_adj_true = CombinedLoss.batch_specific_adj_matrix(img_product_ids).values\n",
    "    if isinstance(sim_matrix, torch.Tensor):\n",
    "        sim_matrix = sim_matrix.detach().numpy()\n",
    "    batch_adj_pred = np.zeros_like(sim_matrix)\n",
    "    batch_adj_pred[sim_matrix.argmax(0), np.arange(sim_matrix.shape[1])] = 1\n",
    "    accuracies = []\n",
    "    for prod_id in range(len(batch_adj_true)):\n",
    "        true_imgs = batch_adj_true[prod_id, :]\n",
    "        pred_imgs = batch_adj_pred[prod_id, :]\n",
    "        # If at least one incorrect image is mapped to the product\n",
    "        # then the accuracy on this product is 0.\n",
    "        if -1 in true_imgs - pred_imgs:\n",
    "            accuracies.append(0)\n",
    "            continue\n",
    "        # If no image is mapped to the product, the accuracy is 0\n",
    "        if len(np.where(pred_imgs==1)[0]) == 0:\n",
    "            accuracies.append(0)\n",
    "            continue\n",
    "        accuracies.append(1)\n",
    "    return accuracies\n",
    "\n",
    "def accuracy_score(model, products_groups, data_set=dataset):\n",
    "    accuracies = []\n",
    "    for i, pr_group in tqdm(enumerate(products_groups)):\n",
    "        im_batch, txt_batch, img_product_ids = data_set.getbatch(pr_group)\n",
    "        im_batch = torch.stack(im_batch, dim=0) # Images are converted to batched tensor\n",
    "\n",
    "        img_product_ids = np.array(img_product_ids) # Numpy array from (image,product) pairs\n",
    "\n",
    "        labels, discovered_products = [], [] \n",
    "        for p, product in enumerate(img_product_ids[:,1]): # In that loop unique text batch is created\n",
    "            if product not in discovered_products:\n",
    "                labels.append(txt_batch[p])\n",
    "                discovered_products.append(product)\n",
    "\n",
    "        batch_accs = _batch_accuracy_score(model, im_batch, labels, img_product_ids)\n",
    "        accuracies += batch_accs\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2215d9-511d-4337-8046-08af1403b864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = JewelryClassifier(128, 2, 0.01, 0.2, 0.1)\n",
    "# model.load_only_model(\"models/model.pth\")\n",
    "# model.accuracy_scores(products_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63a7cc09-9d85-4b72-a355-77d8546d83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56335ed5-79a9-4931-b06d-2f4e6cb0c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [02:36,  6.51s/it]\n"
     ]
    }
   ],
   "source": [
    "accs = accuracy_score(model, products_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a411367f-8d51-484d-9b2d-a7fcea59639a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "702b8a5a-0d00-44a0-8a04-8919b9d18286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                  | 0/24 [00:09<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# That cycle has to be used to receive batches !!!\n",
    "model = JewelryClassifier(128, 2, 0.01, 0.2, 0.1)\n",
    "for i in tqdm(products_groups):\n",
    "    X, Y, IDS = dataset.getbatch(i) # X=images, Y=descriptions of products, IDS=pairs of (image_id,product_id)\n",
    "    images = torch.stack(X, dim=0) # Images are converted to batched tensor\n",
    "    labels = pd.Series(Y).unique() # From descriptions of products only unique ones are  selected, preserving the order\n",
    "    img_product_ids = np.array(IDS)\n",
    "    loss = model.train_emb(images, labels, img_product_ids)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f6424f4-f018-4962-a40d-17bbe59e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, epochs, emb_size, grad_clip=2, use_valid=False, *loss_args):\n",
    "    \"\"\"The main training function.\"\"\"\n",
    "    model = JewelryClassifier(emb_size, grad_clip, *loss_args)\n",
    "    pbar = tqdm(range(1, epochs+1))\n",
    "    avg_loss = \"inf\"\n",
    "    val_loss = \"inf\"\n",
    "    def format_desc():\n",
    "        pbar.set_description(f\"AVG Loss={avg_loss}, Train Loss={loss}, Valid Loss={val_loss}  (epoch {epoch})\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        # TODO: Add learning rate scheduler\n",
    "        losses = []\n",
    "        # pbar = tqdm(enumerate(train_loader))\n",
    "        # for i, (im_batch, txt_batch, img_product_ids) in pbar:\n",
    "        for i, pr_group in tqdm(enumerate(products_groups)):\n",
    "            im_batch, txt_batch, img_product_ids = dataset.getbatch(pr_group)\n",
    "            im_batch = torch.stack(im_batch, dim=0) # Images are converted to batched tensor\n",
    "            txt_batch = pd.Series(txt_batch).unique() # From descriptions of products only unique ones are  selected, preserving the order\n",
    "            img_product_ids = np.array(img_product_ids)\n",
    "            loss = model.train_emb(im_batch, txt_batch, img_product_ids)\n",
    "            losses.append(loss)\n",
    "            format_desc()\n",
    "        avg_loss = loss = sum(losses)/len(losses)\n",
    "        format_desc()\n",
    "        if use_valid:\n",
    "            val_losses = []\n",
    "            for i, (im_batch, txt_batch, txt_lengths) in enumerate(val_loader):\n",
    "                model.eval()\n",
    "                im_emb, txt_emb = model.forward_emb(im_batch, txt_batch, txt_lengths, grad=False)\n",
    "                val_loss = model.forward_loss(im_emb, txt_emb)\n",
    "                val_losses.append(val_loss)\n",
    "                format_desc()\n",
    "            val_loss = sum(val_losses)/len(val_losses)\n",
    "            format_desc()\n",
    "        # TODO Checkpointing\n",
    "    # Save model\n",
    "    model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095aa5a6-6611-4aca-8b0d-8e9efb340f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add accuracy. Products with at least one correct image and without any incorrect ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2ce68d1-d363-4533-b934-2c3ac6b24b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                   | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomImageLoader' object has no attribute 'get_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproducts_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, epochs, emb_size, grad_clip, use_valid, *loss_args)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# pbar = tqdm(enumerate(train_loader))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# for i, (im_batch, txt_batch, img_product_ids) in pbar:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pr_group \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(products_groups)):\n\u001b[0;32m---> 16\u001b[0m     im_batch, txt_batch, img_product_ids \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m(pr_group)\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_emb(im_batch, txt_batch, img_product_ids)\n\u001b[1;32m     18\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomImageLoader' object has no attribute 'get_batch'"
     ]
    }
   ],
   "source": [
    "train(products_groups, None, 2, 128, 2, False, 0.2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47657f-e9a3-4282-b5a0-4333ba92fd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch CPU",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
