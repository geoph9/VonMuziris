{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12647efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba996b",
   "metadata": {},
   "source": [
    "### Preparation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a07b0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts_path, images_path):\n",
    "  \"\"\"Create dataframe with path to image and corresponding text description\"\"\"\n",
    "\n",
    "  texts_df = pd.read_csv(texts_path)\n",
    "  texts_df['text'] = texts_df['color'] + \" \" + texts_df['name'] + \" \" + texts_df['description']\n",
    "  texts_df = texts_df[['Unnamed: 0','text']]\n",
    "  texts_df['product'] = np.arange(len(texts_df))\n",
    "    \n",
    "  df = pd.DataFrame(columns=[\"Image\",\"Text\"])  \n",
    "    \n",
    "  for image in glob.glob(images_path):\n",
    "    img_name = os.path.basename(image)\n",
    "    key_img_name = img_name.split('_')[0]\n",
    "    img_descr = texts_df[texts_df['Unnamed: 0']==int(key_img_name)].iloc[0,1:]\n",
    "    df = df.append({'Image': img_name, 'Text':img_descr[0], 'Product':img_descr[1]}, ignore_index=True)\n",
    "    \n",
    "  return df, df['Product'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d7fba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageLoader:\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = annotations_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def getbatch(self, prod_idx):\n",
    "        batch = []\n",
    "        sliced_indices = self.img_labels[self.img_labels['Product'].isin(prod_idx)].index\n",
    "\n",
    "        for i in sliced_indices:\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[i, 0])\n",
    "            image = Image.open(img_path)\n",
    "            label, product = self.img_labels.iloc[i, 1], self.img_labels.iloc[i, 2]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            batch.append((image,label,product))\n",
    "            \n",
    "        unzipped = list(zip(*batch))\n",
    "        \n",
    "        return unzipped[0], unzipped[1], unzipped[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f89b4-b93f-4d28-a749-1d8916c2a091",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a69a9-08d3-4d62-a339-b0a4a55bf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1 = (t, im1, im2, im3)\n",
    "b_2 = (t2, i1, i2)\n",
    "\n",
    "emb_1 = (t_embs(t), ...)  # (4, emb_size)\n",
    "emb_2 = ... #(3, emb_size)\n",
    "\n",
    "\n",
    "B = (b_1, b_2, b_3, b_4, b_5) # (batch_size, 4, emb_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c97f3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transformation of images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n",
    "])\n",
    "\n",
    "# Data paths\n",
    "descriptions_data = \"./processed_data/processedSKUs_nodups.csv\"\n",
    "images_folder = \"./processed_data/images/*.jpg\"\n",
    "img_dir = \"./processed_data/images/\"\n",
    "\n",
    "# Creation of organized dataframe\n",
    "annotations_file, products = create_dataset(descriptions_data, images_folder)\n",
    "\n",
    "# Creation of custom Batch Loader, where batch contains images, belonging to same product\n",
    "dataset = CustomImageLoader(annotations_file, img_dir, transform=transform)\n",
    "\n",
    "# Creation of batches of products\n",
    "batch_size = 10\n",
    "products_groups = [products[i:i + batch_size] for i in range(0, len(products), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "702b8a5a-0d00-44a0-8a04-8919b9d18286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in products_groups:\n",
    "    X, Y, P = dataset.getbatch(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e98a90-ed64-4919-a561-310843690e10",
   "metadata": {},
   "source": [
    "## Training Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029622bc-607d-4dd8-87d3-ceadadeb25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import FullEncoder, ImageEncoder, TextEncoder\n",
    "from loss import OurLossFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c79bf14-7ff4-4086-b62a-10c061376d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JewelryClassifier:\n",
    "    def __init__(self, emb_size, grad_clip=2, *loss_args):\n",
    "        self.grad_clip = grad_clip\n",
    "        self.im_enc = ImageEncoder(emb_size)\n",
    "        self.txt_enc = TextEncoder(emb_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.im_enc.cuda()\n",
    "            self.txt_enc.cuda()\n",
    "        self.criterion = OurLossFunction(*loss_args)\n",
    "        params = self.txt_enc.fc.parameters()\n",
    "        params += self.im_enc.fc.parameters()\n",
    "        # The image cnn is fine-tuned but roberta is not.\n",
    "        params += self.im_enc.cnn.parameters()\n",
    "        self.params = params\n",
    "        self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "        self.step = 0\n",
    "    \n",
    "    def state_dict(self):\n",
    "        state_dict = [self.im_enc.state_dict(), self.txt_enc.state_dict()]\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.im_enc.load_state_dict(state_dict[0])\n",
    "        self.txt_enc.load_state_dict(state_dict[1])\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def on_stage_start(self, stage):\n",
    "        if stage == \"TRAIN\":\n",
    "            self.im_enc.train()\n",
    "            self.txt_enc.train()\n",
    "        elif stage == \"VALID\":\n",
    "            self.im_enc.eval()\n",
    "            self.txt_enc.eval()\n",
    "    \n",
    "    def train(self):\n",
    "        return self.on_stage_start(\"TRAIN\")\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.on_stage_start(\"VALID\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_tensor(x, grad=True):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.Tensor(x, requires_grad=grad)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return x\n",
    "    \n",
    "    def forward_emb(self, imgs, texts, lengths, grad=True):\n",
    "        \"\"\"Compute the image and text embeddings\n",
    "        \"\"\"\n",
    "        # Set mini-batch dataset\n",
    "        imgs = self.to_tensor(imgs, grad)\n",
    "        txts = self.to_tensor(txts, grad)\n",
    "\n",
    "        # Forward\n",
    "        imgs_emb = self.im_enc(imgs)\n",
    "        txts_emb = self.txt_enc(txts, lengths)\n",
    "        return imgs_emb, txts_emb\n",
    "\n",
    "    def forward_loss(self, imgs_emb, txts_emb):\n",
    "        \"\"\"Compute the loss given pairs of image and text embeddings\n",
    "        \"\"\"\n",
    "        loss = self.criterion(imgs_emb, txts_emb)\n",
    "        return loss\n",
    "\n",
    "    def train_emb(self, imgs, txts, lengths):\n",
    "        \"\"\"One training step given images and captions.\n",
    "        \"\"\"\n",
    "        self.step += 1\n",
    "\n",
    "        # compute the embeddings\n",
    "        imgs_emb, txts_emb = self.forward_emb(imgs, txts, lengths)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.forward_loss(imgs_emb, txts_emb)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        if self.grad_clip > 0:\n",
    "            clip_grad_norm(self.params, self.grad_clip)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6424f4-f018-4962-a40d-17bbe59e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, n_epochs, emb_size, grad_clip=2, *loss_args):\n",
    "    model = JewelryClassifier(emb_size, grad_clip, *loss_args)\n",
    "    pbar = tqdm(range(1, epochs+1))\n",
    "    avg_loss = \"inf\"\n",
    "    val_loss = \"inf\"\n",
    "    def format_desc():\n",
    "        pbar.set_description(f\"AVG Loss={avg_loss}, Train Loss={loss}, Valid Loss={val_loss}  (epoch {epoch})\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        # TODO: Add learning rate scheduler\n",
    "        losses = []\n",
    "        pbar = tqdm(enumerate(train_loader))\n",
    "        for i, (im_batch, txt_batch, txt_lengths) in pbar:\n",
    "            loss = model.train_emb(im_batch, txt_batch, txt_lengths)\n",
    "            losses.append(loss)\n",
    "            format_desc()\n",
    "        avg_loss = loss = sum(losses)/len(losses)\n",
    "        format_desc()\n",
    "        val_losses = []\n",
    "        for i, (im_batch, txt_batch, txt_lengths) in enumerate(val_loader):\n",
    "            model.eval()\n",
    "            im_emb, txt_emb = model.forward_emb(im_batch, txt_batch, txt_lengths, grad=False)\n",
    "            val_loss = model.forward_loss(im_emb, txt_emb)\n",
    "            val_losses.append(val_loss)\n",
    "            format_desc()\n",
    "        val_loss = sum(val_losses)/len(val_losses)\n",
    "        format_desc()\n",
    "        # TODO Checkpointing\n",
    "    # Save model\n",
    "    model.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch CPU",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
