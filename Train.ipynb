{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12647efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba996b",
   "metadata": {},
   "source": [
    "### Preparation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07b0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts_path, images_path):\n",
    "  \"\"\"Create dataframe with path to image and corresponding text description\"\"\"\n",
    "\n",
    "  texts_df = pd.read_csv(texts_path)\n",
    "  texts_df['text'] = texts_df['color'] + \" \" + texts_df['name'] + \" \" + texts_df['description']\n",
    "  texts_df = texts_df[['Unnamed: 0','text']]\n",
    "  texts_df['product'] = np.arange(len(texts_df))\n",
    "    \n",
    "  df = pd.DataFrame(columns=[\"Image\",\"Text\"])  \n",
    "    \n",
    "  for image in glob.glob(images_path):\n",
    "    img_name = os.path.basename(image)\n",
    "    key_img_name = img_name.split('_')[0]\n",
    "    img_descr = texts_df[texts_df['Unnamed: 0']==int(key_img_name)].iloc[0,1:]\n",
    "    df = df.append({'Image': img_name, 'Text':img_descr[0], 'Product':img_descr[1]}, ignore_index=True)\n",
    "    \n",
    "  return df, df['Product'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7fba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageLoader:\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = annotations_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def getbatch(self, prod_idx):\n",
    "        batch = []\n",
    "        sliced_indices = self.img_labels[self.img_labels['Product'].isin(prod_idx)].index\n",
    "\n",
    "        for i in sliced_indices:\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[i, 0])\n",
    "            image = Image.open(img_path)\n",
    "            label, product = self.img_labels.iloc[i, 1], self.img_labels.iloc[i, 2]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            batch.append((image,label,product))\n",
    "            \n",
    "        unzipped = list(zip(*batch))\n",
    "        \n",
    "        return unzipped[0], unzipped[1], unzipped[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c97f3a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transformation of images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n",
    "])\n",
    "\n",
    "# Data paths\n",
    "descriptions_data = \"./processed_data/processedSKUs_nodups.csv\"\n",
    "images_folder = \"./processed_data/images/*.jpg\"\n",
    "img_dir = \"./processed_data/images/\"\n",
    "\n",
    "# Creation of organized dataframe\n",
    "annotations_file, products = create_dataset(descriptions_data, images_folder)\n",
    "\n",
    "# Creation of custom Batch Loader, where batch contains images, belonging to same product\n",
    "dataset = CustomImageLoader(annotations_file, img_dir, transform=transform)\n",
    "\n",
    "# Creation of batches of products\n",
    "prod_batch_size = 10\n",
    "products_groups = [products[i:i + batch_size] for i in range(0, len(products), prod_batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80e50d",
   "metadata": {},
   "source": [
    "### Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eddcd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    \"\"\"L2-normalization of features columns\"\"\"\n",
    "    norm = torch.pow(X, 2).sum(dim=1, keepdim=True).sqrt()\n",
    "    X = torch.div(X, norm)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88a3d5b0-e0fd-4068-a65f-2a010918d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size, cnn_type):\n",
    "        \"\"\"Initializing parameters\"\"\"\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.embedding_size = embedding_size # Size of projected image\n",
    "        self.cnn = self.load_cnn(cnn_type)\n",
    "\n",
    "        # No need to finetune parameters = frozen layers\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replacing last fully connected layer with new one\n",
    "        self.fc = nn.Linear(self.cnn.classifier._modules['6'].in_features, embedding_size)\n",
    "        self.cnn.classifier = nn.Sequential(*list(self.cnn.classifier.children())[:-1])\n",
    "\n",
    "        # Initializing the weights of fully-connected layer, which makes projection to new space\n",
    "        self.initialization_weights()\n",
    "  \n",
    "    def load_cnn(self, cnn_type):\n",
    "        \"\"\"Loading pretrained model\"\"\"\n",
    "        model = models.__dict__[cnn_type](pretrained=True)\n",
    "        print(model)\n",
    "        return model\n",
    "\n",
    "    def initialization_weights(self):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        r = np.sqrt(6.) / np.sqrt(self.fc.in_features + self.fc.out_features)\n",
    "        self.fc.weight.data.uniform_(-r, r)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Creation of features\"\"\"\n",
    "        # Creation of embeddings\n",
    "        features = self.cnn(X)\n",
    "\n",
    "        # Normalization of embeddings\n",
    "        features = normalization(features)\n",
    "\n",
    "        # Projection to new space\n",
    "        features = self.fc(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e98a90-ed64-4919-a561-310843690e10",
   "metadata": {},
   "source": [
    "## Training Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "029622bc-607d-4dd8-87d3-ceadadeb25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import FullEncoder, ImageEncoder, TextEncoder\n",
    "#from loss import OurLossFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c79bf14-7ff4-4086-b62a-10c061376d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JewelryClassifier:\n",
    "    def __init__(self, emb_size=128, grad_clip=2, *loss_args):\n",
    "        self.grad_clip = grad_clip\n",
    "        self.im_enc = ImageEncoder(emb_size, 'vgg19')\n",
    "        self.txt_enc = TextEncoder(emb_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.im_enc.cuda()\n",
    "            self.txt_enc.cuda()\n",
    "        self.criterion = OurLossFunction(*loss_args)\n",
    "        params = self.txt_enc.fc.parameters()\n",
    "        params += self.im_enc.fc.parameters()\n",
    "        # The image cnn is fine-tuned but roberta is not.\n",
    "        params += self.im_enc.cnn.parameters()\n",
    "        self.params = params\n",
    "        self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "        self.step = 0\n",
    "    \n",
    "    def state_dict(self):\n",
    "        state_dict = [self.im_enc.state_dict(), self.txt_enc.state_dict()]\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.im_enc.load_state_dict(state_dict[0])\n",
    "        self.txt_enc.load_state_dict(state_dict[1])\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def on_stage_start(self, stage):\n",
    "        if stage == \"TRAIN\":\n",
    "            self.im_enc.train()\n",
    "            self.txt_enc.train()\n",
    "        elif stage == \"VALID\":\n",
    "            self.im_enc.eval()\n",
    "            self.txt_enc.eval()\n",
    "    \n",
    "    def train(self):\n",
    "        return self.on_stage_start(\"TRAIN\")\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.on_stage_start(\"VALID\")\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def to_tensor(x, grad=True):\n",
    "    #     if not isinstance(x, torch.Tensor):\n",
    "    #         x = torch.Tensor(x, requires_grad=grad)\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         x = x.cuda()\n",
    "    #     return x\n",
    "    \n",
    "    def forward_emb(self, imgs, texts, lengths=None, grad=True):\n",
    "        \"\"\"Compute the image and text embeddings\n",
    "        \"\"\"\n",
    "        # Set mini-batch dataset\n",
    "        #imgs = self.to_tensor(imgs, grad)\n",
    "        #txts = self.to_tensor(txts, grad)\n",
    "\n",
    "        # Forward\n",
    "        imgs_emb = self.im_enc(imgs)\n",
    "        txts_emb = self.txt_enc(txts, lengths)\n",
    "        return imgs_emb, txts_emb\n",
    "\n",
    "    def forward_loss(self, imgs_emb, txts_emb, product_ids):\n",
    "        \"\"\"Compute the loss given pairs of image and text embeddings\n",
    "        \"\"\"\n",
    "        loss = self.criterion(imgs_emb, txts_emb, product_ids)\n",
    "        return loss\n",
    "\n",
    "    def train_emb(self, imgs, txts, product_ids):\n",
    "        \"\"\"One training step given images and captions.\n",
    "        \"\"\"\n",
    "        self.step += 1\n",
    "\n",
    "        # compute the embeddings\n",
    "        imgs_emb, txts_emb = self.forward_emb(imgs, txts, lengths)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.forward_loss(imgs_emb, txts_emb, product_ids)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        if self.grad_clip > 0:\n",
    "            clip_grad_norm(self.params, self.grad_clip)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "702b8a5a-0d00-44a0-8a04-8919b9d18286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in products_groups:\n",
    "    X, Y, P = dataset.getbatch(i)\n",
    "    images = torch.stack(X, dim=0)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6424f4-f018-4962-a40d-17bbe59e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, n_epochs, emb_size, grad_clip=2, *loss_args):\n",
    "    model = JewelryClassifier(emb_size, grad_clip, *loss_args)\n",
    "    pbar = tqdm(range(1, epochs+1))\n",
    "    avg_loss = \"inf\"\n",
    "    val_loss = \"inf\"\n",
    "    def format_desc():\n",
    "        pbar.set_description(f\"AVG Loss={avg_loss}, Train Loss={loss}, Valid Loss={val_loss}  (epoch {epoch})\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        # TODO: Add learning rate scheduler\n",
    "        losses = []\n",
    "        pbar = tqdm(enumerate(train_loader))\n",
    "        for i, (im_batch, txt_batch, product_ids) in pbar:\n",
    "            loss = model.train_emb(im_batch, txt_batch, product_ids)\n",
    "            losses.append(loss)\n",
    "            format_desc()\n",
    "        avg_loss = loss = sum(losses)/len(losses)\n",
    "        format_desc()\n",
    "        val_losses = []\n",
    "        for i, (im_batch, txt_batch, txt_lengths) in enumerate(val_loader):\n",
    "            model.eval()\n",
    "            im_emb, txt_emb = model.forward_emb(im_batch, txt_batch, txt_lengths, grad=False)\n",
    "            val_loss = model.forward_loss(im_emb, txt_emb)\n",
    "            val_losses.append(val_loss)\n",
    "            format_desc()\n",
    "        val_loss = sum(val_losses)/len(val_losses)\n",
    "        format_desc()\n",
    "        # TODO Checkpointing\n",
    "    # Save model\n",
    "    model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9306e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d8ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb065ce5-18dd-46a5-b67b-5b9ea593a7b6",
   "metadata": {},
   "source": [
    "# Loss:\n",
    "1. Define similarity matrix.\n",
    "2. Define the true adjacency matrix.\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
