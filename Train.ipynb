{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12647efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba996b",
   "metadata": {},
   "source": [
    "### Preparation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a07b0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts_path, images_path):\n",
    "  \"\"\"Create dataframe with path to image and corresponding text description\"\"\"\n",
    "\n",
    "  texts_df = pd.read_csv(texts_path)\n",
    "  texts_df['text'] = texts_df['color'] + \" \" + texts_df['name'] + \" \" + texts_df['description']\n",
    "  texts_df = texts_df[['Unnamed: 0','text']]\n",
    "  texts_df['product'] = np.arange(len(texts_df))\n",
    "    \n",
    "  df = pd.DataFrame(columns=[\"Image\",\"Text\"])  \n",
    "    \n",
    "  for image in glob.glob(images_path):\n",
    "    img_name = os.path.basename(image)\n",
    "    key_img_name = img_name.split('_')[0]\n",
    "    img_descr = texts_df[texts_df['Unnamed: 0']==int(key_img_name)].iloc[0,1:]\n",
    "    df = df.append({'Image': img_name, 'Text':img_descr[0], 'Product':img_descr[1]}, ignore_index=True)\n",
    "    \n",
    "  return df, df['Product'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d7fba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageLoader:\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = annotations_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def getbatch(self, prod_idx):\n",
    "        batch = []\n",
    "        sliced_indices = self.img_labels[self.img_labels['Product'].isin(prod_idx)].index\n",
    "\n",
    "        for i in sliced_indices:\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[i, 0])\n",
    "            image = Image.open(img_path)\n",
    "            label, product = self.img_labels.iloc[i, 1], self.img_labels.iloc[i, 2]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            batch.append((image,label,product))\n",
    "            \n",
    "        unzipped = list(zip(*batch))\n",
    "        \n",
    "        return unzipped[0], unzipped[1], unzipped[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f89b4-b93f-4d28-a749-1d8916c2a091",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a69a9-08d3-4d62-a339-b0a4a55bf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1 = (t, im1, im2, im3)\n",
    "b_2 = (t2, i1, i2)\n",
    "\n",
    "emb_1 = (t_embs(t), ...)  # (4, emb_size)\n",
    "emb_2 = ... #(3, emb_size)\n",
    "\n",
    "\n",
    "B = (b_1, b_2, b_3, b_4, b_5) # (batch_size, 4, emb_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c97f3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transformation of images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n",
    "])\n",
    "\n",
    "# Data paths\n",
    "descriptions_data = \"./processed_data/processedSKUs_nodups.csv\"\n",
    "images_folder = \"./processed_data/images/*.jpg\"\n",
    "img_dir = \"./processed_data/images/\"\n",
    "\n",
    "# Creation of organized dataframe\n",
    "annotations_file, products = create_dataset(descriptions_data, images_folder)\n",
    "\n",
    "# Creation of custom Batch Loader, where batch contains images, belonging to same product\n",
    "dataset = CustomImageLoader(annotations_file, img_dir, transform=transform)\n",
    "\n",
    "# Creation of batches of products\n",
    "batch_size = 10\n",
    "products_groups = [products[i:i + batch_size] for i in range(0, len(products), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "702b8a5a-0d00-44a0-8a04-8919b9d18286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in products_groups:\n",
    "    X, Y, P = dataset.getbatch(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbf5a6-049c-42a1-be8c-ce32a3501f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
