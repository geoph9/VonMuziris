{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12647efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchtext.models import ROBERTA_BASE_ENCODER\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de76196c-304f-4ae3-b5ae-363a3620d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79392b5c-39c3-4f96-aac0-cf422f37bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Seed. Credits: https://clay-atlas.com/us/blog/2021/08/24/pytorch-en-set-seed-reproduce/\n",
    "seed = 1001\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba996b",
   "metadata": {},
   "source": [
    "### 1. Preparation of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1180ca",
   "metadata": {},
   "source": [
    "The unified dataset with rows for each pair of image names and corresponding texts is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07b0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts_path, images_path):\n",
    "    \"\"\"Create dataframe with path to image and corresponding text description\"\"\"\n",
    "\n",
    "    texts_df = pd.read_csv(texts_path)\n",
    "    texts_df['text'] = texts_df['color'] + \" \" + texts_df['name'] + \" \" + texts_df['description']\n",
    "    texts_df = texts_df[['Unnamed: 0','text']]\n",
    "    texts_df['product'] = np.arange(len(texts_df))\n",
    "    df = pd.DataFrame(columns=[\"Image_id\", \"Image_name\",\"Text\",\"Product_id\"])  \n",
    "    \n",
    "    for i, image in enumerate(glob.glob(images_path)):\n",
    "        img_name = os.path.basename(image)\n",
    "        key_img_name = img_name.split('_')[0]\n",
    "        img_descr = texts_df[texts_df['Unnamed: 0']==int(key_img_name)].iloc[0,1:]\n",
    "        df = df.append({'Image_id':i, 'Image_name': img_name, 'Text':img_descr[0], 'Product_id':img_descr[1]}, ignore_index=True)\n",
    "    \n",
    "    return df, df['Product_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab63624",
   "metadata": {},
   "source": [
    "The class CustomImageLoader helps to build custom loader of the training data in the form of the batches. It takes into account that all the images, corresponding to the same product have to be in the same batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageLoader:\n",
    "    \"\"\"Creation of batches of images, texts and their ids.\"\"\"\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, batch_size=10):\n",
    "        self.img_labels = annotations_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def getbatch(self, prod_idx):\n",
    "        batch = []\n",
    "        sliced_indices = self.img_labels[self.img_labels['Product_id'].isin(prod_idx)].index\n",
    "\n",
    "        for i in sliced_indices:\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[i, 1])\n",
    "            image = Image.open(img_path)\n",
    "            image_id, text, product_id = self.img_labels.iloc[i, 0], self.img_labels.iloc[i, 2], self.img_labels.iloc[i, 3]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            batch.append((image,text,(image_id, product_id)))\n",
    "            \n",
    "        unzipped = list(zip(*batch))\n",
    "        \n",
    "        return unzipped[0], unzipped[1], unzipped[2]\n",
    "    \n",
    "    def __getitem__(self, prod_idx):\n",
    "        return self.getbatch(prod_idx)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        products_groups = [products[i:i + self.batch_size] for i in range(0, len(products), self.batch_size)]\n",
    "        for i in products_groups:\n",
    "            yield self.getbatch(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c3778",
   "metadata": {},
   "source": [
    "Preparation steps for passing data to dataloader are made, as well as, the constant variables for the whole program are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97f3a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transformation of images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n",
    "])\n",
    "\n",
    "# Data paths (Arina)\n",
    "# descriptions_data = \"./processed_data/processedSKUs_nodups.csv\"\n",
    "# images_folder = \"./processed_data/images/*.jpg\"\n",
    "# img_dir = \"./processed_data/images/\"\n",
    "# George\n",
    "descriptions_data = \"../processedSKUs_nodups.csv\"\n",
    "images_folder = \"../images_onlyids/*.jpg\"\n",
    "img_dir = \"../images_onlyids/\"\n",
    "\n",
    "# Creation of organized dataframe\n",
    "annotations_file, products = create_dataset(descriptions_data, images_folder)\n",
    "\n",
    "# Creation of true constant adjacency matrix, where rows - products, columns - images. \n",
    "ADJACENCY_TRUE = pd.crosstab(annotations_file.Product_id, annotations_file.Image_id)\n",
    "\n",
    "# Creation of custom Batch Loader, where batch contains images, belonging to same product\n",
    "dataset = CustomImageLoader(annotations_file, img_dir, transform=transform)\n",
    "\n",
    "# Creation of batches of products. For each product there are 2/3 images, so the actual batch size is ~20-30 pairs.\n",
    "prod_batch_size = 10\n",
    "products_groups = [products[i:i + prod_batch_size] for i in range(0, len(products), prod_batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80e50d",
   "metadata": {},
   "source": [
    "### 2. Definition of classes for image, text encoders and supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eddcd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    \"\"\"L2-normalization of features columns\"\"\"\n",
    "    norm = torch.pow(X, 2).sum(dim=1, keepdim=True).sqrt()\n",
    "    X = torch.div(X, norm)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a3d5b0-e0fd-4068-a65f-2a010918d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size, cnn_type):\n",
    "        \"\"\"Initializing parameters\"\"\"\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.embedding_size = embedding_size # Size of projected image\n",
    "        self.cnn = self.load_cnn(cnn_type)\n",
    "\n",
    "        # No need to finetune parameters = frozen layers\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replacing last fully connected layer with new one\n",
    "        self.fc = nn.Linear(self.cnn.classifier._modules['6'].in_features, embedding_size)\n",
    "        self.cnn.classifier = nn.Sequential(*list(self.cnn.classifier.children())[:-1])\n",
    "\n",
    "        # Initializing the weights of fully-connected layer, which makes projection to new space\n",
    "        self.initialization_weights()\n",
    "  \n",
    "    def load_cnn(self, cnn_type):\n",
    "        \"\"\"Loading pretrained model\"\"\"\n",
    "        model = models.__dict__[cnn_type](pretrained=True)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def initialization_weights(self):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        r = np.sqrt(6.) / np.sqrt(self.fc.in_features + self.fc.out_features)\n",
    "        self.fc.weight.data.uniform_(-r, r)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Creation of features\"\"\"\n",
    "        # Creation of embeddings\n",
    "        features = self.cnn(X)\n",
    "\n",
    "        # Normalization of embeddings\n",
    "        features = normalization(features)\n",
    "\n",
    "        # Projection to new space\n",
    "        features = self.fc(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3e6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBERTA_OUT_DIM = 768\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"Initializing parameters\"\"\"\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.embedding_size = embedding_size # Size of projected text\n",
    "        self.roberta = ROBERTA_BASE_ENCODER.get_model()\n",
    "        self.transform = ROBERTA_BASE_ENCODER.transform()\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(ROBERTA_OUT_DIM, embedding_size)\n",
    "\n",
    "        # Initializing the weights of fully-connected layer, which makes projection to new space\n",
    "        self.initialization_weights()\n",
    "        \n",
    "    def _roberta_encode(self, batch):\n",
    "        transformed = self.transform(batch)\n",
    "        model_input = to_tensor(transformed, padding_value=1)\n",
    "        return self.roberta(model_input)\n",
    "\n",
    "    def initialization_weights(self):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        r = np.sqrt(6.) / np.sqrt(self.fc.in_features + self.fc.out_features)\n",
    "        self.fc.weight.data.uniform_(-r, r)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        \"\"\"Creation of features\"\"\"\n",
    "        # Creation of embeddings\n",
    "        features = self._roberta_encode(list(X))\n",
    "\n",
    "        # Normalization of embeddings\n",
    "        features = normalization(features)\n",
    "\n",
    "        # Projection to new space\n",
    "        features = self.fc(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8470af7",
   "metadata": {},
   "source": [
    "### 3. Definition of loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f30e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(img, text):\n",
    "    \"\"\"Similarity calculation\"\"\"\n",
    "    \n",
    "    return text.mm(img.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311d09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Class for combined loss\"\"\"\n",
    "\n",
    "    def __init__(self, margin=0, lambda_coeff=0.1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "        # self.sim = similarity_score\n",
    "    \n",
    "    def _calc_penalty(self, similarities):\n",
    "        if isinstance(similarities, torch.Tensor):\n",
    "            similarities = similarities.detach().numpy()\n",
    "        adj_tmp = np.zeros_like(similarities)\n",
    "        adj_tmp[similarities.argmax(0), np.arange(similarities.shape[1])] = 1\n",
    "        images_mapped = adj_tmp.sum(1)\n",
    "        assert images_mapped.sum() == similarities.shape[1], f\"{images_mapped.shape=}, {similarities.shape=}\"\n",
    "        penalty = self.lambda_coeff * len(np.where(images_mapped==0)[0])\n",
    "        return penalty\n",
    "\n",
    "    def forward(self, image_emb, label_emb, img_product_ids):\n",
    "        # Image_emb - batch_size x embedding size\n",
    "        # Label_emb - number of unique products from batch x embedding size\n",
    "        # Img_product_ids - batch_size x 2, where in each row there is (image_index, product_index).\n",
    "        batch_size = image_emb.shape[0]\n",
    "        products_num = label_emb.shape[0]\n",
    "\n",
    "        # Slicing from true adjacency matrix, prepared beforehand\n",
    "        adj_true = ADJACENCY_TRUE.iloc[img_product_ids[:,1]]\n",
    "        adj_true = adj_true.iloc[:,img_product_ids[:,0]]\n",
    "        \n",
    "        adj_true = adj_true[~adj_true.index.duplicated(keep=\"first\")]\n",
    "        # print(adj_true.sum(0).values)\n",
    "        # Creation of similarity matrix of size - number of unique products from batch x batch_size\n",
    "        sim_matrix = similarity_score(image_emb, label_emb)\n",
    "        \n",
    "        # Calculation of loss for each product\n",
    "        # 1st - For each product minimum similarity with true images is calculated\n",
    "        adj_true = torch.Tensor(adj_true.values)\n",
    "        product_true = sim_matrix*adj_true\n",
    "        product_min_sims = product_true.masked_fill(product_true == 0, np.inf).min(dim=1)[0]\n",
    "        \n",
    "        # 2nd - For each product maximum similarity score with image, not belonging to it\n",
    "        adj_true_inv = torch.where(adj_true==0, 1, 0)\n",
    "        product_false = sim_matrix*adj_true_inv\n",
    "        product_max_sims, product_max_sims_indices = product_false.max(axis=1)\n",
    "        \n",
    "        # 3d - For each product maximum similarity between its images and products, other than the true one, is found\n",
    "        not_product_max_sims = np.array([])\n",
    "        for p in range(products_num):\n",
    "            sim_p = (sim_matrix[np.arange(products_num)!=p,:]*adj_true[p,:]).max()\n",
    "            not_product_max_sims = np.append(not_product_max_sims,sim_p.detach())\n",
    "        not_product_max_sims = torch.Tensor(not_product_max_sims)\n",
    "        \n",
    "        # Caluculation of mean loss\n",
    "        first_hinge = self.margin+not_product_max_sims - product_min_sims\n",
    "        second_hinge = self.margin+product_max_sims - product_min_sims\n",
    "        loss = torch.where(first_hinge<0,0,first_hinge)+torch.where(second_hinge<0,0,second_hinge)\n",
    "        loss = torch.sum(loss)/products_num\n",
    "        #Regularizer (this is just a single value)\n",
    "        loss += self._calc_penalty(sim_matrix)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e98a90-ed64-4919-a561-310843690e10",
   "metadata": {},
   "source": [
    "### 4. Definition of class and functions for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddd83e",
   "metadata": {},
   "source": [
    "The JewelryClassifier class combines the procedure of creation of the embeddings for images and texts, as well as, calculates loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c79bf14-7ff4-4086-b62a-10c061376d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JewelryClassifier:\n",
    "    \"\"\"Class, unifying the embeddings creation and loss calculation.\"\"\"\n",
    "    def __init__(self, emb_size=128, grad_clip=2, learning_rate=0.01, *loss_args, **loss_kwargs):\n",
    "        print(f\"{learning_rate=}, {loss_args=}\")\n",
    "        self.grad_clip = grad_clip\n",
    "        self.im_enc = ImageEncoder(emb_size, 'vgg19')\n",
    "        self.txt_enc = TextEncoder(emb_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.im_enc.cuda()\n",
    "            self.txt_enc.cuda()\n",
    "        self.criterion = CombinedLoss(*loss_args, **loss_kwargs)\n",
    "        params = list(self.txt_enc.fc.parameters())\n",
    "        params += list(self.im_enc.fc.parameters())\n",
    "        # The image cnn is fine-tuned but roberta is not.\n",
    "        params += self.im_enc.cnn.parameters()\n",
    "        self.params = params\n",
    "        self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "        self.step = 0\n",
    "    \n",
    "    def state_dict(self):\n",
    "        state_dict = [self.im_enc.state_dict(), self.txt_enc.state_dict()]\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.im_enc.load_state_dict(state_dict[0])\n",
    "        self.txt_enc.load_state_dict(state_dict[1])\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def on_stage_start(self, stage):\n",
    "        if stage == \"TRAIN\":\n",
    "            self.im_enc.train()\n",
    "            self.txt_enc.train()\n",
    "        elif stage == \"VALID\":\n",
    "            self.im_enc.eval()\n",
    "            self.txt_enc.eval()\n",
    "    \n",
    "    def train(self):\n",
    "        return self.on_stage_start(\"TRAIN\")\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.on_stage_start(\"VALID\")\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def to_tensor(x, grad=True):\n",
    "    #     if not isinstance(x, torch.Tensor):\n",
    "    #         x = torch.Tensor(x, requires_grad=grad)\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         x = x.cuda()\n",
    "    #     return x\n",
    "    \n",
    "    def forward_emb(self, imgs, txts, lengths=None, grad=True):\n",
    "        \"\"\"Compute the image and text embeddings\"\"\"\n",
    "        # Set mini-batch dataset\n",
    "        #imgs = self.to_tensor(imgs, grad)\n",
    "        #txts = self.to_tensor(txts, grad)\n",
    "\n",
    "        # Forward\n",
    "        imgs_emb = self.im_enc(imgs)\n",
    "        txts_emb = self.txt_enc(txts, lengths)\n",
    "        return imgs_emb, txts_emb\n",
    "\n",
    "    def forward_loss(self, imgs_emb, txts_emb, img_product_ids):\n",
    "        \"\"\"Compute the loss given pairs of image and text embeddings and there ids\"\"\"\n",
    "        loss = self.criterion(imgs_emb, txts_emb, img_product_ids)\n",
    "        return loss\n",
    "\n",
    "    def train_emb(self, imgs, txts, img_product_ids):\n",
    "        \"\"\"One training step given images and captions.\"\"\"\n",
    "        self.step += 1\n",
    "\n",
    "        # compute the embeddings\n",
    "        imgs_emb, txts_emb = self.forward_emb(imgs, txts)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.forward_loss(imgs_emb, txts_emb.mean(axis=1), img_product_ids)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        if self.grad_clip > 0:\n",
    "            clip_grad_norm(self.params, self.grad_clip)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244669ac-10f2-4ca4-afa7-94c8f70a3589",
   "metadata": {},
   "source": [
    "## Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2498610-dea1-440d-a915-a4959e56aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_accuracy_score(model, imgs, txts, img_product_ids):\n",
    "    model.eval()\n",
    "    # compute the embeddings\n",
    "    imgs_emb, txts_emb = model.forward_emb(imgs, txts)\n",
    "    # sim_matrix: rows=products, columns=images\n",
    "    sim_matrix = similarity_score(imgs_emb, txts_emb.mean(axis=1))\n",
    "    # The accuracy score is the normalized number of products that\n",
    "    # have been mapped to at least one correct image and no incorrect images.\n",
    "    # NOTE: This is on the batch level so it's not normalized. Check the accuracy_score\n",
    "    #       function for the normalized version.\n",
    "    batch_adj_true = CombinedLoss.batch_specific_adj_matrix(img_product_ids).values\n",
    "    if isinstance(sim_matrix, torch.Tensor):\n",
    "        sim_matrix = sim_matrix.detach().numpy()\n",
    "    batch_adj_pred = np.zeros_like(sim_matrix)\n",
    "    batch_adj_pred[sim_matrix.argmax(0), np.arange(sim_matrix.shape[1])] = 1\n",
    "    accuracies = []\n",
    "    for prod_id in range(len(batch_adj_true)):\n",
    "        true_imgs = batch_adj_true[prod_id, :]\n",
    "        pred_imgs = batch_adj_pred[prod_id, :]\n",
    "        # If at least one incorrect image is mapped to the product\n",
    "        # then the accuracy on this product is 0.\n",
    "        if -1 in true_imgs - pred_imgs:\n",
    "            accuracies.append(0)\n",
    "            continue\n",
    "        # If no image is mapped to the product, the accuracy is 0\n",
    "        if len(np.where(pred_imgs==1)[0]) == 0:\n",
    "            accuracies.append(0)\n",
    "            continue\n",
    "        accuracies.append(1)\n",
    "    return accuracies\n",
    "\n",
    "def accuracy_score(model, products_groups, data_set=dataset):\n",
    "    accuracies = []\n",
    "    for i, pr_group in tqdm(enumerate(products_groups)):\n",
    "        im_batch, txt_batch, img_product_ids = data_set.getbatch(pr_group)\n",
    "        im_batch = torch.stack(im_batch, dim=0) # Images are converted to batched tensor\n",
    "\n",
    "        img_product_ids = np.array(img_product_ids) # Numpy array from (image,product) pairs\n",
    "\n",
    "        labels, discovered_products = [], [] \n",
    "        for p, product in enumerate(img_product_ids[:,1]): # In that loop unique text batch is created\n",
    "            if product not in discovered_products:\n",
    "                labels.append(txt_batch[p])\n",
    "                discovered_products.append(product)\n",
    "\n",
    "        batch_accs = _batch_accuracy_score(model, im_batch, labels, img_product_ids)\n",
    "        accuracies += batch_accs\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f6424f4-f018-4962-a40d-17bbe59e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(products_groups, val_loader, epochs, emb_size, grad_clip=2, use_valid=False, lr=0.01, *loss_args, **loss_kwargs):\n",
    "    \"\"\"The main training function.\"\"\"\n",
    "    model = JewelryClassifier(emb_size, grad_clip, lr, *loss_args, **loss_kwargs)\n",
    "    val_loss = \"null\"\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for i, pr_group in tqdm(enumerate(products_groups)):\n",
    "            im_batch, txt_batch, img_product_ids = dataset.getbatch(pr_group)\n",
    "            im_batch = torch.stack(im_batch, dim=0) # Images are converted to batched tensor\n",
    "\n",
    "            img_product_ids = np.array(img_product_ids) # Numpy array from (image,product) pairs\n",
    "            \n",
    "            labels, discovered_products = [], [] \n",
    "            for p, product in enumerate(img_product_ids[:,1]): # In that loop unique text batch is created\n",
    "                if product not in discovered_products:\n",
    "                    labels.append(txt_batch[p])\n",
    "                    discovered_products.append(product)\n",
    "            \n",
    "            loss = model.train_emb(im_batch, labels, img_product_ids)\n",
    "            losses.append(loss)\n",
    "        # print(f\"In epoch {epoch} the average loss is {np.mean(losses)}.\")\n",
    "        \n",
    "        if use_valid:\n",
    "            val_losses = []\n",
    "            for i, (im_batch, txt_batch, txt_lengths) in enumerate(val_loader):\n",
    "                model.eval()\n",
    "                im_emb, txt_emb = model.forward_emb(im_batch, txt_batch, txt_lengths, grad=False)\n",
    "                val_loss = model.forward_loss(im_emb, txt_emb)\n",
    "                val_losses.append(val_loss)\n",
    "                format_desc()\n",
    "            val_loss = sum(val_losses)/len(val_losses)\n",
    "            format_desc()\n",
    "\n",
    "    torch.save(model, 'model.pth')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805d4ff-b46b-41e4-9205-3fc3da4a9311",
   "metadata": {},
   "source": [
    "## Loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3c66ea-0095-4649-831b-151365bab870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4806a8be-6e31-44bd-85d7-5c94687de638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(losses, set_type=\"Train\", out_path=None):\n",
    "    plt.plot(losses)\n",
    "    plt.title(f\"{set_type} loss over the epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    if out_path is not None:\n",
    "        plt.savefig(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf58d1-95cc-4901-94c4-4d1449d50717",
   "metadata": {},
   "source": [
    "## Train the model and get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce68d1-d363-4533-b934-2c3ac6b24b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate=0.01, loss_args=(0.2, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train_losses = train(products_groups, None, 8, 128, 2, False, 0.01, 0.2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc913a66-4669-4822-a7da-a91e804bb588",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_losses, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47657f-e9a3-4282-b5a0-4333ba92fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1de23-de71-46ff-9d2c-b4ca3ffa30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = accuracy_score(model, products_groups)\n",
    "print(\"Final Accuracy:\", accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch CPU",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
